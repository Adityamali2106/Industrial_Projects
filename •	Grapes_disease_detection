import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.decomposition import PCA
from sklearn.preprocessing import LabelEncoder
import os
from PIL import Image

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
import tensorflow.keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, SpatialDropout2D, Activation, Lambda, Flatten, LSTM
from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras import utils

import os
import shutil

# Define the source and destination directories
src_dir = '/content/drive/MyDrive/project dataset'
test_dir = 'test'
train_dir = 'train'
valid_dir = 'valid'

# Create the destination directories
os.makedirs(test_dir, exist_ok=True)
os.makedirs(train_dir, exist_ok=True)
os.makedirs(valid_dir, exist_ok=True)

# Loop through each class folder (0-9)
for class_folder in os.listdir(src_dir):
    # Create the class folder in each destination directory
    os.makedirs(os.path.join(test_dir, class_folder), exist_ok=True)
    os.makedirs(os.path.join(train_dir, class_folder), exist_ok=True)
    os.makedirs(os.path.join(valid_dir, class_folder), exist_ok=True)

    # Get the list of image files in the class folder
    img_files = os.listdir(os.path.join(src_dir, class_folder))

    # Filter out directories from the list of image files
    img_files = [f for f in img_files if os.path.isfile(os.path.join(src_dir, class_folder, f))]

    # Split the image files into test, train, and valid sets
    test_size = int(0.2 * len(img_files))  # 20% for testing
    valid_size = int(0.1 * len(img_files))  # 10% for validation
    train_size = len(img_files) - test_size - valid_size

    # Copy the image files to the corresponding destination directories
    for i, img_file in enumerate(img_files):
        if i < test_size:
            shutil.copy2(os.path.join(src_dir, class_folder, img_file),
                           os.path.join(test_dir, class_folder, img_file))
        elif i < test_size + valid_size:
            shutil.copy2(os.path.join(src_dir, class_folder, img_file),
                           os.path.join(valid_dir, class_folder, img_file))
        else:
            shutil.copy2(os.path.join(src_dir, class_folder, img_file),
                           os.path.join(train_dir, class_folder, img_file))

print("Libraries Imported")

# Define paths to the training, validation, and test datasets
train_folder = '/content/train'
test_folder = '/content/test'
validate_folder = '/content/valid'

# Define paths to the specific classes within the dataset
Black_Mold = '/content/drive/MyDrive/project dataset/Black Mold'
Downy_Mildew = '/content/drive/MyDrive/project dataset/Downy Mildew'
Mosaic_Virus_Disease = '/content/drive/MyDrive/project dataset/Mosaic Virus Disease'
Powdery_Mildew = '/content/drive/MyDrive/project dataset/Powdery Mildew'
Sour_Rot_Disease = '/content/drive/MyDrive/project dataset/Sour Rot Disease'
Ulcer_Disease = '/content/drive/MyDrive/project dataset/Ulcer Disease'
Healthy = '/content/drive/MyDrive/project dataset/Healthy'
Gray_Mold= '/content/drive/MyDrive/project dataset/Gray Mold'

# Set the image size for resizing
IMAGE_SIZE = (350, 350)

# Initialize the image data generators for training and testing
train_datagen = ImageDataGenerator(
    rescale=1./255,
    horizontal_flip=True,
    rotation_range=20,
    zoom_range=0.2,
    shear_range=0.2,
    fill_mode='nearest'
)
test_datagen = ImageDataGenerator(rescale=1./255)

# Define the batch size for training
batch_size = 32

# Create the training data generator
train_generator = train_datagen.flow_from_directory(
    train_folder,
    target_size=IMAGE_SIZE,
    batch_size=batch_size,
    color_mode="rgb",
    class_mode='categorical'
)

# Set up callbacks for learning rate reduction, early stopping, and model checkpointing
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint

learning_rate_reduction = ReduceLROnPlateau(
    monitor='val_loss',
    patience=3,
    verbose=1,
    factor=0.5,
    min_lr=0.00001
)
early_stops = EarlyStopping(
    monitor='val_loss',
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode='auto',
    restore_best_weights=True
)

checkpointer = ModelCheckpoint(
    filepath='best_densenet_model.weights.h5',
    verbose=1,
    save_best_only=True,
    save_weights_only=True
)

# Define the number of output classes
OUTPUT_SIZE = 8

# Load a pre-trained DenseNet model without the top layers and freeze its weights
# You can choose from DenseNet121, DenseNet169, DenseNet201
pretrained_model = tf.keras.applications.DenseNet121(
    weights='imagenet',
    include_top=False,
    input_shape=[*IMAGE_SIZE, 3]
)

# Freeze the pre-trained layers
for layer in pretrained_model.layers:
    layer.trainable = False

print("Pretrained DenseNet model used:")
pretrained_model.summary()

# Create a new model with the pre-trained base and additional layers for classification
model = Sequential([
    pretrained_model,
    GlobalAveragePooling2D(),
    Dense(512, activation='relu'),
    Dropout(0.3),
    Dense(OUTPUT_SIZE, activation='softmax')
])

print("Final model created:")
model.summary()

# Print the batch shape
batch_x, batch_y = next(iter(train_generator))
print(f"Batch shapes - X: {batch_x.shape}, y: {batch_y.shape}")

# Create the validation data generator
validation_generator = test_datagen.flow_from_directory(
    validate_folder,
    target_size=IMAGE_SIZE,
    batch_size=batch_size,
    color_mode="rgb",
    class_mode='categorical'
)

# Create the test data generator
test_generator = test_datagen.flow_from_directory(
    test_folder,
    target_size=IMAGE_SIZE,
    batch_size=batch_size,
    color_mode="rgb",
    class_mode='categorical',
    shuffle=False
)

# Compile the model before training
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the Model
history = model.fit(
    train_generator,
    epochs=10,  # Increased epochs for better learning
    validation_data=validation_generator,
    callbacks=[learning_rate_reduction, early_stops, checkpointer]
)

print("Final training accuracy =", history.history['accuracy'][-1])
print("Final validation accuracy =", history.history['val_accuracy'][-1])

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(test_generator)
print(f"Test accuracy: {test_accuracy:.4f}")

# Function to display training curves for loss and accuracy
def display_training_curves(training, validation, title, subplot):
    if subplot % 10 == 1:
        plt.subplots(figsize=(12, 10), facecolor='#F0F0F0')
        plt.tight_layout()
    ax = plt.subplot(subplot)
    ax.set_facecolor('#F8F8F8')
    ax.plot(training)
    ax.plot(validation)
    ax.set_title('model ' + title)
    ax.set_ylabel(title)
    ax.set_xlabel('epoch')
    ax.legend(['train', 'valid.'])

# Display training curves for loss and accuracy
display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 211)
display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 212)

# Save the trained model
model.save('/content/drive/MyDrive/Model/densenet_disease_model.h5')

# Function to load and preprocess an image for prediction
from tensorflow.keras.preprocessing import image
import numpy as np

def load_and_preprocess_image(img_path, target_size):
    img = image.load_img(img_path, target_size=target_size)
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array /= 255.0  # Rescale the image like the training images
    return img_array

# Load, preprocess, and predict the class of an image
img_path = '/content/drive/MyDrive/project dataset/Black Mold/IMG_0573.JPG'
img = load_and_preprocess_image(img_path, IMAGE_SIZE)
predictions = model.predict(img)
predicted_class = np.argmax(predictions[0])
class_labels = list(train_generator.class_indices.keys())
predicted_label = class_labels[predicted_class]

print(class_labels)
print(f"Predicted class index: {predicted_class}")
print(f"The image belongs to class: {predicted_label}")

# Display the image with the predicted class
plt.figure(figsize=(8, 8))
plt.imshow(image.load_img(img_path, target_size=IMAGE_SIZE))
plt.title(f"Predicted: {predicted_label}", fontsize=16)
plt.axis('off')
plt.show()

# Create a confusion matrix to evaluate model performance
from sklearn.metrics import confusion_matrix, classification_report

# Get the true labels
y_true = test_generator.classes

# Predict on the test set
y_pred_prob = model.predict(test_generator)
y_pred = np.argmax(y_pred_prob, axis=1)

# Create and display confusion matrix
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(12, 10))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# Print classification report
print("Classification Report:")
print(classification_report(y_true, y_pred, target_names=class_labels))
